{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "import pyconll.util\n",
    "import re\n",
    "import nltk\n",
    "import wikipedia\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Page infomation from wikipedia dataset\n",
    "title = ['Barack Obama','Donald Trump','Kobe Bryant']\n",
    "dataset = [wikipedia.page(t) for t in title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing  Barack Obama\n",
      "Finishing 0th link\n",
      "Finishing 100th link\n",
      "Finishing 200th link\n",
      "Finishing 300th link\n",
      "Finishing 400th link\n",
      "Finishing 500th link\n",
      "Finishing 600th link\n",
      "Finishing 700th link\n",
      "Finishing 800th link\n",
      "Finishing 900th link\n",
      "Finishing 1000th link\n",
      "Finishing 1100th link\n",
      "Finishing 1200th link\n",
      "Finishing 1300th link\n",
      "Finishing 1400th link\n",
      "Finishing 1500th link\n",
      "Finishing 1600th link\n",
      "Finishing 1700th link\n",
      "Finishing 1800th link\n",
      "Finishing 1900th link\n",
      "Finishing 2000th link\n",
      "Preprocessing  Donald Trump\n",
      "Finishing 0th link\n",
      "Finishing 100th link\n",
      "Finishing 200th link\n",
      "Finishing 300th link\n",
      "Finishing 400th link\n",
      "Finishing 500th link\n",
      "Finishing 600th link\n",
      "Finishing 700th link\n",
      "Finishing 800th link\n",
      "Finishing 900th link\n",
      "Finishing 1000th link\n",
      "Finishing 1100th link\n",
      "Finishing 1200th link\n",
      "Finishing 1300th link\n",
      "Finishing 1400th link\n",
      "Finishing 1500th link\n",
      "Preprocessing  Kobe Bryant\n",
      "Finishing 0th link\n",
      "Finishing 100th link\n",
      "Finishing 200th link\n",
      "Finishing 300th link\n",
      "Finishing 400th link\n",
      "Finishing 500th link\n",
      "Finishing 600th link\n",
      "Finishing 700th link\n"
     ]
    }
   ],
   "source": [
    "# Extract all the sentences that contains link\n",
    "pages = collections.defaultdict(dict)\n",
    "for d in dataset:\n",
    "    print('Preprocessing ', d.title)\n",
    "    for i,link in enumerate(d.links):\n",
    "        if i % 100 == 0:\n",
    "            print('Finishing {0}th link'.format(i))\n",
    "        if link in d.content:\n",
    "            index = d.content.index(link)\n",
    "            start_index = index\n",
    "            end_index = index + len(link)\n",
    "            while start_index > 0:\n",
    "                if d.content[start_index] in '.!?=\\n\\t':\n",
    "                    \n",
    "                    tmpIndex = start_index\n",
    "                    while d.content[tmpIndex] != ' ':\n",
    "                        tmpIndex -= 1\n",
    "                    found = False\n",
    "                    for l in d.links:\n",
    "                        if d.content[tmpIndex+1:start_index+1] in l:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found:\n",
    "                        start_index -= 1\n",
    "                        continue\n",
    "                        \n",
    "                    start_index += 1\n",
    "                    break\n",
    "                start_index -= 1\n",
    "            while end_index < len(d.content):\n",
    "                if d.content[end_index] in '.!?=\\n\\t':\n",
    "                    \n",
    "                    tmpIndex = end_index\n",
    "                    while d.content[tmpIndex] != ' ':\n",
    "                        tmpIndex -= 1\n",
    "                    found = False\n",
    "                    for l in d.links:\n",
    "                        if d.content[tmpIndex+1:end_index+1] in l:\n",
    "                            found = True\n",
    "                            break;\n",
    "                    if found:\n",
    "                        end_index += 1\n",
    "                        continue\n",
    "                        \n",
    "                    break\n",
    "                end_index += 1\n",
    "            sentence = d.content[start_index:end_index+1]\n",
    "            pages[d.title][link] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences containing links:  923\n"
     ]
    }
   ],
   "source": [
    "# Check the number of sentences we need to preprocess\n",
    "print('Number of Sentences containing links: ',sum(len(sentences[s]) for s in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = collections.defaultdict(list)\n",
    "# Tokenize each sentence and pos tag them\n",
    "for p in pages:\n",
    "    for s in pages[p]:\n",
    "        sentence = pages[p][s]\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        upos = nltk.pos_tag(tokens,tagset='universal')\n",
    "        xpos = nltk.pos_tag(tokens)\n",
    "        for i,u in enumerate(upos):\n",
    "            if u[1] == '.':\n",
    "                u[1] == 'PUNCT'\n",
    "        sentences[p].append([upos,xpos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out the file \n",
    "f = open('wiki.conll',\"w+\")\n",
    "for s in sentences:\n",
    "    f.write(\"# Wikipedia Page for {0}\\n\".format(s))\n",
    "    l = sentences[s]\n",
    "    for pos in l:\n",
    "        upos = pos[0]\n",
    "        xpos = pos[1]\n",
    "        for i,p in enumerate(upos):\n",
    "            f.write(\"{0}\\t{1}\\t_\\t{2}\\t{3}\\t_\\t_\\t_\\t_\\t_\\n\".format(i+1,upos[i][0],upos[i][1],xpos[i][1]))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
